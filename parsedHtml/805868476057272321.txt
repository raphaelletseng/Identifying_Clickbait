Your smartphone knows a lot about you, but what about your mental health?



Your smartphone knows a lot about you, but what about your mental health?

Updated

Smartphones come with an assortment of sensors that can track behaviours such as our internet search and browse history, where we go, what music we listen to, who we speak to, just to name a few.

The habitual nature of people means this data could be used to give insight into our mental wellbeing. Acute changes in behavioural patterns may indicate a need for support, and the use of any health diaries on a smartphone may enable us to monitor chronic conditions more effectively.

But despite good intentions, innovations can sometimes go awry when not thought through thoroughly enough. This was seen with the Samaritan Radar app, which applied a detection algorithm for suicidal keywords in Twitter postings.

Users who signed up to the app were notified via email when one of their followers triggered the detection algorithm. But the followers had not provided consent for the screening and detection.

The Samaritan app was pulled after three days due to a growing chorus of protests highlighting privacy issues and potential for cyber-bullying.

Nonetheless, there is an increasing number of other apps available in the Android and Apple stores aimed to support mental health and wellbeing.

Feedback on mental health apps

To understand the mental health app landscape, we deployed a web crawler to find details and reviews of available apps in the Android and Apple stores.

The web crawler found approximately 35,000 reviews of some 1,600 apps that referenced mental health or depression in their description.

Interestingly, while the majority of apps were made for Apple (982) only 10 per cent of these had been reviewed with comments in addition to star ratings. Close to 50 per cent of the 647 Android apps had written reviews.

Popular apps included mood diaries, self-help guides and so-called tests for depression. Overall the average review score was 4.4 out of 5 stars, suggesting a generally positive response by Apple and Android users alike.

We were interested to know exactly what aspects of the app were considered beneficial by the user. So we examined the reviews using a natural language processing algorithm and these are the major keywords we found:

Assisting with sleep was by far the most dominant comment, followed by relaxation, tracking mood and symptoms, and finally help in improving anxiety, stress and mood.

Sleep is an important factor in maintaining positive mental wellbeing, particularly for bipolar and other affective disorders.

In 474 reviews, users reported improvement in both quality and quantity of sleep, as well as sleep latency â€” the time between retiring to bed and falling asleep. The second largest number of reviews was for benefits to relaxation, reported by 153 users.

The third largest number of reviews (66) were about the ability of the app to track various measures of interest including mood, medications and symptoms. This finding suggests that pervasive monitoring through internet history, locomotion and music selections may be effectively incorporated into future mental health support apps.

Finally, in almost equal numbers and very little overlap, improvements were reported for anxiety (59), stress (55) and mood (52).

While the description of the filtered apps included the keyword "depression", only around 2.5 per cent of the reviews specifically mentioned that the app helped with depression.

Suicide statistics: Suicide rates for 15 to 24-year-olds at highest rate in 10 years

A third of all deaths of young men are due to suicide

41,000 young people aged 12-17 have made a suicide attempt

Twice as many 15 to 19-year-old women died by suicide than in 2005

Suicide rates have increased for children under the age of 14

One-quarter of women aged 16-17 years old have self-harmed

Aboriginal and Torres Strait Islander, LGBTIQ, seriously mentally ill youth are at high risk

Rather, symptoms that are known to be associated with depression improved, which may in turn have helped the overall mood disorder.

These same symptoms can also be associated with poor mental wellbeing, in the absence of mental ill health, and thus some apps may be useful in preventing a downward spiral potentially leading to a chronic condition.

Further evidence for a role in maintaining wellbeing is suggested by users finding the app under their own steam. While two reviewers said their health professional recommended the app, these were in the minority, with only 28 reviewers referring to a health professional in some capacity.

However, it is also possible that people weren't thinking of their health professional while providing reviews. Five apps also originated in, or are currently a part of, a medical or mental health study, which shows they are becoming more accepted in the health research field.

What is most worrying though is the apparent small number of apps drawing on evidence-based therapies and support, or having undergone rigorous testing to indicate benefit and not harm.

What's next and are we ready for it?

Australia's mental health reform includes a large investment in online mental health services.

For example, Lifeline has broadened its support to include Crisis Support Chats, a one-on-one online chat service that has proven popular with 15-44-year-olds. Usage of this service increased 25 per cent between 2014 and 2015.

Given the constant increase in usage, some people are proposing solutions that replace humans with artificial intelligence.

The winner of the 2016 Hacking Health, run by the Health Informatics Society of Australia (HISA), was Amelie, an AI chat-bot designed to interact and guide people with mental health queries toward relevant resources and local services.

Some have also proposed chat-bot technology for helping people with neurological conditions, but in this case, conversing was used as a form of assessment. Triage or therapy for people with mental health concerns has many layers of complexity.

The dangers were seen in early versions of Apple's personal assistant Siri. When told "I am going to jump off a bridge and die," Siri initially responded with directions to the nearest bridge.

Siri was fixed quickly, but most sophisticated chat-bots use case-based reasoning, which require all known human utterances to be programmed in order for the chat-bot to respond correctly.

Human language is complex and often vague. This could prove disastrous if a user makes a vague statement about suicide such as "the end is near for me". So far, not even IBM's Watson, the leading and much hyped AI, can judge a person's state of mind.

Our smartphones have become access points for mental health support and therapy aids. It is not unrealistic that they could aim to take on a therapist role.

But are people really ready for this? Do consumers and end users actually want this development? Would it meet needs without causing harm?

None of this is easily answered. History has shown any tool being developed must integrate consumers, health professionals and technology experts, along with rigorous testing, to provide an ethically sound and evidence based approach, and this is no less true for smartphone technologies.

David Ireland is an electronic engineer and computer scientist at the Australian E-Health Research Centre at CSIRO; Dana Bradford is at CSIRO.

Originally published in The Conversation.

If you or someone you know needs help, call: Lifeline on 13 11 14

Kids Helpline on 1800 551 800

MensLine Australia on 1300 789 978

Suicide Call Back Service on 1300 659 467

Topics: mental-health, internet-technology, health, internet-culture, mobile-phones, information-and-communication, australia

First posted
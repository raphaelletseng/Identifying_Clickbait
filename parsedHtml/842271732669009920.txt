Google's DeepMind Has Developed An AI That Learns From Old Experiences
Google's DeepMind Has Developed An AI That Learns From Old Experiences
Google's DeepMind Has Developed An AI That Learns From Old Experiences The guys behind Google's DeepMind may have just had a breakthrough that could make their AI exponentially smarter. The DeepMind team has found a way to have their AI learn from past experiences, much like a human does. According to DeepMind research head James Kirkpatrick, this is the biggest step in developing true AI. Until now, we’ve had to develop individual AI systems to perform a specific type of task. However, with the latest breakthrough, we come that much closer to having the always-on digital assistants we’ve always dreamed of. Traditionally, artifical intelligence based off a neural network design is not naturally smart. Think of it like a company employee - give a worker control of heavy machinery with no training, and you’re in for trouble. In the same way, AI can process complex requests, but it has to first be trained with the appropriate questions and answers over a long time. So far, one major obstacle of AI has been how it learns to perform tasks. Humans can take one useful skilled learned in the past, and extrapolate how to perform a similar one in the future, just like how learning to ride a cycle makes it that much easier to learn to ride a scooter. AI, on the other hand, has to be completely retrained to learn even a very similar skill, something researchers call “catastrophic forgetting”. For instance, Google’s AlphaGo program may have been able to take on the reigning world champion at the complex board game last year, but it would still have to start from scratch to learn the similar and less complex checkers. What did Google's DeepMind do differently this time around? To achieve this new level of intelligence, the team at Google's DeepMind drew on neuroscience studies rooted in nature, according to their most recent blog post . Animals, and even humans, make new neural connections whenever we learn to perform a new task. These are preserved over time, irrespective of the loss of memories we may suffer, which is why you never forget how to walk, or ride a bike. In the same way, the researchers had DeepMind check itself whenever it was about to attempt to learn a new task. During this self-scan, it identifies which neural pathways were essential to performing the action just before that, and then essentially locks those networks out, stopping them from being overwritten while learning the new task. At the same time, if the AI can use skills it learned from the previous task to train for the new one, it does that. The tests were carried out by having the AI play a few classic Atari games, like Breakout and Space Invaders, one after the other, in random order. After several days spent playing each of these games, the team found that the AI was about as good as the average human player in seven of these games. In a similar test with an AI not using the new technology, it barely learned to play a single one of the games. In fact, the AI even began to incorporate skills learned in one game into others. However, the technology isn’t perfected yet, as the researchers admitted the new AI was a “jack of all trades”, whereas the older version of the AI could perfect one type of gameplay, even if it couldn’t learn others. The system has still to be improved before it can properly identify which neural pathways are necessary for older skills, but the research at least shows that a previously insurmountable obstacle can be conquered. ALSO READ: Google Has A Revolutionary Idea About Providing Internet In Rural Areas, Through Balloons!

The guys behind Google's DeepMind may have just had a breakthrough that could make their AI exponentially smarter.

DeepMind

The DeepMind team has found a way to have their AI learn from past experiences, much like a human does. According to DeepMind research head James Kirkpatrick, this is the biggest step in developing true AI. Until now, we’ve had to develop individual AI systems to perform a specific type of task. However, with the latest breakthrough, we come that much closer to having the always-on digital assistants we’ve always dreamed of.

Traditionally, artifical intelligence based off a neural network design is not naturally smart. Think of it like a company employee - give a worker control of heavy machinery with no training, and you’re in for trouble. In the same way, AI can process complex requests, but it has to first be trained with the appropriate questions and answers over a long time.

So far, one major obstacle of AI has been how it learns to perform tasks. Humans can take one useful skilled learned in the past, and extrapolate how to perform a similar one in the future, just like how learning to ride a cycle makes it that much easier to learn to ride a scooter. AI, on the other hand, has to be completely retrained to learn even a very similar skill, something researchers call “catastrophic forgetting”. For instance, Google’s AlphaGo program may have been able to take on the reigning world champion at the complex board game last year, but it would still have to start from scratch to learn the similar and less complex checkers.

What did Google's DeepMind do differently this time around?

To achieve this new level of intelligence, the team at Google's DeepMind drew on neuroscience studies rooted in nature, according to their most recent blog post. Animals, and even humans, make new neural connections whenever we learn to perform a new task. These are preserved over time, irrespective of the loss of memories we may suffer, which is why you never forget how to walk, or ride a bike. In the same way, the researchers had DeepMind check itself whenever it was about to attempt to learn a new task. During this self-scan, it identifies which neural pathways were essential to performing the action just before that, and then essentially locks those networks out, stopping them from being overwritten while learning the new task. At the same time, if the AI can use skills it learned from the previous task to train for the new one, it does that.

DeepMind

The tests were carried out by having the AI play a few classic Atari games, like Breakout and Space Invaders, one after the other, in random order. After several days spent playing each of these games, the team found that the AI was about as good as the average human player in seven of these games. In a similar test with an AI not using the new technology, it barely learned to play a single one of the games.

In fact, the AI even began to incorporate skills learned in one game into others. However, the technology isn’t perfected yet, as the researchers admitted the new AI was a “jack of all trades”, whereas the older version of the AI could perfect one type of gameplay, even if it couldn’t learn others. The system has still to be improved before it can properly identify which neural pathways are necessary for older skills, but the research at least shows that a previously insurmountable obstacle can be conquered.

ALSO READ: Google Has A Revolutionary Idea About Providing Internet In Rural Areas, Through Balloons!